{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install memory_profiler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imports\nimport tensorflow as tf # Tensorflow 2.3\nimport tensorflow_hub as hub # Tensorflow-hub 0.12\nimport PIL.Image as Image # Pillow \nimport numpy as np # Numpy\nfrom tqdm import tqdm # progress bar package\nimport tensorflow_datasets as tfds\nimport os\nfrom time import process_time\nfrom memory_profiler import memory_usage\nimport tempfile\nimport tracemalloc\nimport matplotlib.image as mpimg\nimport pandas as pd\nimport PIL.Image as Image # Pillow\nimport math\n\n################################################################################################################################################################\n\n# Opening the text file and saving to the corresponding lists.\ndef load_values(file): \n    val_groundtruth = []\n    with open(file, 'r') as f:\n        val_set = f.read().splitlines()\n    for line in val_set:\n        # Image ground truth.\n        ground_truth = line.split(' ')[1]\n        val_groundtruth.append(int(ground_truth))\n    return val_groundtruth\n\n################################################################################################################################################################\n\n# Image Preprocessing\ndef prepare(path, size):\n    im = Image.open(path)\n    im = im.convert('RGB')\n    re_size = int(round(1.14286*size))\n    \n    width, height = im.size\n    new_height = height * re_size // min(width,height)\n    new_width = width * re_size // min(width,height)\n    im = im.resize((new_width,new_height))\n\n    left = math.floor((new_width - size)/2)\n    top = math.ceil((new_height - size)/2)\n    right = math.floor((new_width + size)/2)\n    bottom = math.ceil((new_height + size)/2)\n\n\n    im = im.crop((left, top, right, bottom))\n    input = (np.array(im))/255\n    return input\n\n################################################################################################################################################################\n\n# Ellipse defined by the labels of DAGM\ndef calc_ellipse(x, y,label):\n    # https://www.maa.org/external_archive/joma/Volume8/Kalman/General.html\n    ...\n    [semi_major, semi_minor, rotation, x_centre, y_centre] = label\n    term1 = (((x - x_centre) * np.cos(rotation)) + (\n        (y - y_centre) * np.sin(rotation)))**2\n    term2 = (((x - x_centre) * np.sin(rotation)) - (\n        (y - y_centre) * np.cos(rotation)))**2\n    ellipse = ((term1 / semi_major**2) + (term2 / semi_minor**2)) <= 1\n    return ellipse\n\n##################################################################################################################################################################\n\n# IoU calculation\ndef iou_calc(labimg, pred):\n\n    pred = np.round(np.squeeze(pred),decimals = 6).astype(dtype=bool)\n    intersection = np.logical_and(labimg, pred)\n    union = np.logical_or(labimg, pred)\n    iou_score = np.sum(intersection) / np.sum(union)\n    return iou_score\n\n###################################################################################################################################################################\n\n# Model Inference\ndef modelInference(model_list,end):\n\n    [link,batchshape,dataset,background] = model_list\n    \n    # Load Model\n\n    model = tf.keras.Sequential([\n    hub.KerasLayer(link)])\n    model.build(batchshape)\n    batch = batchshape[0]\n    \n    # Inference and metrics\n\n    imagenum = 0\n    latency = []\n    avg_lat = []\n    data = np.zeros(tuple(batchshape))\n    tracemalloc.start()\n\n    for i in range(end):\n        data[imagenum%batch,:] = np.random.rand(batchshape[1],batchshape[2],batchshape[3]) # Preprocessing\n        imagenum += 1\n\n        if (imagenum%batch == 0):\n            start = process_time()    \n            prediction = model.predict(data,batch_size = batch)\n            latency.append(process_time()-start)\n            avg_lat.append(latency[-1]/batch)\n            data = np.zeros(tuple(batchshape))\n\n    if (imagenum%batch != 0):\n        start = process_time()    \n        prediction = model.predict(data[:(imagenum%batch)],batch_size = batch)\n        latency.append(process_time()-start)\n        avg_lat.append(latency[-1]/(imagenum%batch))\n\n    memory = round(((tracemalloc.get_traced_memory()[1])/1024)) #peak memory usage   \n    throughput = float((imagenum)/sum(latency)) \n    tracemalloc.stop()\n\n    return {'memory':memory,'avg(latency)':avg_lat,'latency':latency,'throughput':throughput}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import traceback\n\n#task = input(\"Choose a model by typing the corresponding number: \\n1.MobileNet V2\\n2.ResNet50 V1\\n3.Inception V3\\n4.NASNet-A large\\n5.EfficientNet V2 M\")\n#batch = int(input(\"Insert batch size\"))\n#if (batch == 1): batch = None\n\n# Parameters of each model\n# Format = [link,batch_input_shape,datasetPath,isBackgroundIncluded]\nMobileNet = [\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\",[1,224,224,3],'../Datasets/ImageNet',1]\nResNet = [\"https://tfhub.dev/tensorflow/resnet_50/classification/1\",[1,224,224,3],'../Datasets/ImageNet',0]\nInception = [\"https://tfhub.dev/google/imagenet/inception_v3/classification/5\",[1,299,299,3],'../Datasets/ImageNet',1]\nNASNet = [\"https://tfhub.dev/google/imagenet/nasnet_large/classification/5\",[1,331,331,3],'../Datasets/ImageNet',1]\nEfficientNetM = [\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/classification/2\",[1,480,480,3],'../Datasets/ImageNet',0]\n\nNASNetMob = [\"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/5\",[1,224,224,3],'',0]\nEfficientNetB3 = [\"https://tfhub.dev/google/efficientnet/b3/classification/1\",[1,300,300,3],'',0]\nEfficientNetB4 = [\"https://tfhub.dev/google/efficientnet/b4/classification/1\",[1,380,380,3],'',0]\nEfficientNetS = [\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",[1,384,384,3],'',0]\n\n\nmodel_dict = {'1':MobileNet,'2':ResNet,'3':Inception,'4':NASNet,'5':EfficientNetM,'6':NASNetMob, '7':EfficientNetB3,'8':EfficientNetB4,'9':EfficientNetS}\n\nfor task in [str(i) for i in range(1,10)]:\n\n    filename = 'batches'+task+'CPUK.txt'\n\n    for batch in [2**i for i in range(6)]:\n      \n        model_dict[task][1][0] = batch\n      \n        try:\n\n            #Warmup\n            modelInference(model_dict[task],20)\n\n            inf_dict = modelInference(model_dict[task],300)\n            inf_dict['batch'] = batch\n            f = open(filename, \"a\")\n            f.write(str(inf_dict)+'\\n')\n            f.close()\n            print(\"For batchsize = {}:\\n\\tThoughput = {}samples/sec\\n\\tLatency(All) = {}sec\\n------\".format(batch,inf_dict['throughput'],sum(inf_dict['latency'])))\n        \n        except Exception:\n\n            print(traceback.format_exc())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.gridspec import SubplotSpec\n\ndata = pd.DataFrame(columns=['memory','avglat','tlat','throughput','batchsize','model'])\n\nplt.figure(figsize=(20,10))\n\nfileCPU = open('batches9CPUK.txt', 'r')\nlinesCPU = fileCPU.readlines()\nfileCPU.close()\n\nfileGPU = open('batches9GPUK.txt', 'r')\nlinesGPU = fileGPU.readlines()\nfileGPU.close()\n\n\ntlatCPU = []\navglatCPU = []\nthroughputCPU = []\nbatchCPU = []\nmemoryCPU = []\n\ntlatGPU = []\navglatGPU = []\nthroughputGPU = []\nbatchGPU = []\nmemoryGPU = []\n\n\nfor line in linesCPU:\n    temp = ast.literal_eval(line)\n    memoryCPU.append(temp['memory']) #peak memory per total inference\n    avglatCPU.append(((temp['batch']*sum(temp['latency'][:-1]))+((300%temp['batch'])*temp['latency'][-1]))/300)\n    tlatCPU.append(sum(temp['latency'])) #total latency\n    throughputCPU.append(float(300/sum(temp['latency'])))\n    batchCPU.append(temp['batch'])\n\nfor line in linesGPU:\n    temp = ast.literal_eval(line)\n    memoryGPU.append(temp['memory']) #peak memory per total inference\n    avglatGPU.append(((temp['batch']*sum(temp['latency'][:-1]))+((300%temp['batch'])*temp['latency'][-1]))/300)\n    tlatGPU.append(sum(temp['latency'])) #total latency\n    throughputGPU.append(float(300/sum(temp['latency'])))\n    batchGPU.append(temp['batch'])\n\n  #data = data.append(pd.DataFrame({'memory':memory,'avglat':avglat,'tlat':tlat,'throughput':throughput,'batchsize':batch,'model':np.full(len(batch), filename[-5])}),ignore_index=True)\n\n\n    # create 5x1 subfigs\n    #subfigs = fig.subplots(5, 4)\n    #models = ['MobileNet','ResNet','InceptionNet','NASNet','EfficientNet']\nbatch = batchCPU\nmemory = [memoryCPU,memoryGPU]\nthroughput = [throughputCPU,throughputGPU]\navglat = [avglatCPU,avglatGPU]\ntlat = [tlatCPU,tlatGPU]\n\nfor j in range(2):\n\n    axs = plt.subplot(2, 4, 1+(4*j))\n    axs.bar(batch,memory[j])\n    axs.set_title('memory(batchsize)')\n    axs.set_xlabel('Batchsize')\n    axs.set_ylabel('Peak Memory [kB]')\n\n    axs = plt.subplot(2, 4, 2+(4*j))\n    axs.plot(throughput[j],avglat[j],c='orange',marker='o')\n    axs.set_title('throughput - latency tradeoff')\n    axs.set_xlabel('Throughput')\n    axs.set_ylabel('Latency')\n\n    axs = plt.subplot(2, 4, 3+(4*j))\n    axs.plot(batch,avglat[j],c='green',marker='o')\n    axs.set_title('Average latency per inference')\n    axs.set_xlabel('Batchsize')\n    axs.set_ylabel('Latency')\n\n    axs = plt.subplot(2, 4, 4+(4*j))\n    axs.plot(batch,tlat[j],c='red',marker='o')\n    axs.set_title('Total latency per inference')\n    axs.set_xlabel('Batchsize')\n    axs.set_ylabel('Total Latency')","metadata":{},"execution_count":null,"outputs":[]}]}